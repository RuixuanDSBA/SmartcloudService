{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR1wFmPUFYF7",
        "outputId": "23770766-92f3-4d61-a8e8-7f30ed870d6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/gdrive/MyDrive/JR_project\")"
      ],
      "metadata": {
        "id": "T1LIkZVxF5rU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# 机器学习\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "import lightgbm as lgb\n",
        "\n",
        "\n",
        "# 可视化\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#https://www.kaggle.com/code/shun2741/baseline"
      ],
      "metadata": {
        "id": "o6BsikQJGZvi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read csv file to dataframe\n",
        "aisles = pd.read_csv('aisles.csv')\n",
        "departments = pd.read_csv('departments.csv')\n",
        "orders = pd.read_csv('orders.csv')\n",
        "products = pd.read_csv('products.csv')\n",
        "order_products_train = pd.read_csv('order_products__train.csv')\n",
        "order_products_prior = pd.read_csv('order_products__prior.csv')"
      ],
      "metadata": {
        "id": "r1B3BVP8Ggqk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "特征生成"
      ],
      "metadata": {
        "id": "aZHZq1R9HoLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_features(df, orders_df, orders_product_prior_df):\n",
        "    \n",
        "    # 用于创建聚合功能的表\n",
        "    order_mereged_df = pd.merge(orders_df, orders_product_prior_df, on='order_id', how='inner')\n",
        "    \n",
        "    # 用户×产品列\n",
        "    order_mereged_df['user-product'] = order_mereged_df['user_id'].astype(str) + \\\n",
        "                                        \"-\" + \\\n",
        "                                        order_mereged_df['product_id'].astype(str) \n",
        "    \n",
        "    # df 保存添加的功能\n",
        "    feature_df = df.copy()\n",
        "    \n",
        "    # 用户×产品列\n",
        "    feature_df['user-product'] = feature_df['user_id'].astype(str) + \\\n",
        "                                    \"-\" + \\\n",
        "                                    feature_df['product_id'].astype(str) \n",
        "    ######################### 按产品划分的特征 ###############################\n",
        "    # 1-1.按产品划分的出现次数\n",
        "    print(\"making product_count features\")\n",
        "    product_count_df = order_mereged_df.groupby(\"product_id\").count()[['order_id']].reset_index().rename(\n",
        "        columns={'order_id':'product_count'})\n",
        "    \n",
        "    # 添加您创建的功能\n",
        "    feature_df = pd.merge(feature_df, product_count_df, on='product_id', how='left')\n",
        "    \n",
        "    # 删除中间生成的 df 并释放内存\n",
        "    del product_count_df\n",
        "    gc.collect() \n",
        "\n",
        "    # 1-2. 按产品划分的重新订单率\n",
        "    print(\"making product_reordered_rate features\")\n",
        "    product_reordered_df = order_mereged_df.groupby(\"product_id\").mean()[['reordered']].reset_index().rename(\n",
        "        columns={'reordered':'product_reordered_rate'})\n",
        "    \n",
        "    # 添加您创建的功能\n",
        "    feature_df = pd.merge(feature_df, product_reordered_df, on='product_id', how='left')\n",
        "    \n",
        "    # 删除中间生成的 df 并释放内存\n",
        "    del product_reordered_df\n",
        "    gc.collect() \n",
        "\n",
        "    ######################### 每个用户的特征 ###############################\n",
        "    # 2-1.用户重新购买率\n",
        "    print(\"making user_reorder_rate features\")\n",
        "    user_reorder_rate_df = order_mereged_df.groupby(\"user_id\").mean()[[\"reordered\"]].reset_index().rename(\n",
        "        columns={'reordered':'user_reorder_rate'})\n",
        "    \n",
        "    # 添加您创建的功能\n",
        "    feature_df = pd.merge(feature_df, user_reorder_rate_df, on='user_id', how='left')\n",
        "    \n",
        "    # 删除中间生成的 df 并释放内存\n",
        "    del user_reorder_rate_df\n",
        "    gc.collect()\n",
        "\n",
        "    # 2-2. 用户过去的购买行为\n",
        "    print(\"making user_action_mean features\")\n",
        "    action_list = [\"order_number\", \"order_dow\", \"order_hour_of_day\", \"days_since_prior_order\"]\n",
        "    user_past_action_df = orders_df.groupby(\"user_id\").mean()[action_list].reset_index().rename(\n",
        "        columns={'order_number':'order_number_mean',\n",
        "                 'order_dow':'order_dow_mean',\n",
        "                 'order_hour_of_day':'order_hour_of_day_mean',\n",
        "                 'days_since_prior_order':'days_since_prior_order_mean'})\n",
        "    \n",
        "    # 添加您创建的功能\n",
        "    feature_df = pd.merge(feature_df, user_past_action_df, on='user_id', how='left')\n",
        "    \n",
        "    # 删除中间生成的 df 并释放内存\n",
        "    del user_past_action_df\n",
        "    gc.collect()\n",
        "\n",
        "    # 2-3. 用户过去的订单数\n",
        "    print(\"making user_action_count features\")\n",
        "    user_past_action_df = orders_df.groupby(\"user_id\").count()['order_id'].reset_index().rename(\n",
        "        columns={'order_id':'order_count'})\n",
        "    \n",
        "    # 作成した特徴を追加\n",
        "    feature_df = pd.merge(feature_df, user_past_action_df, on='user_id', how='left')\n",
        "    \n",
        "    # 中間生成したdfを削除しメモリ解放\n",
        "    del user_past_action_df\n",
        "    gc.collect()\n",
        "    \n",
        "    ######################### 每个用户和产品的特点 ###############################\n",
        "    # 3-1.用户是否已经重新订购了产品？\n",
        "    print(\"making reordered features\")\n",
        "    reorder_product = order_mereged_df.query(\"reordered == 1\")[['user_id', 'product_id']].drop_duplicates()\n",
        "    reorder_product['reordered'] = 1\n",
        "\n",
        "    # 添加您创建的功能\n",
        "    feature_df = pd.merge(feature_df, reorder_product, on=['user_id', 'product_id'], how='left')\n",
        "    \n",
        "    # 补充在0，因为产品，从来没有重新订购成为缺陷处理\n",
        "    feature_df['reordered'].fillna(0, inplace=True)\n",
        "    \n",
        "    # 中間生成したdfを削除しメモリ解放\n",
        "    del reorder_product\n",
        "    gc.collect()\n",
        "    \n",
        "    # 3-2. 该用户订购该商品的次数\n",
        "    print(\"making user-product-count features\")\n",
        "    user_product_count_df = order_mereged_df.groupby(\"user-product\").count()[[\"order_id\"]].reset_index().rename(\n",
        "        columns={'order_id':'user-product_count'})\n",
        "    \n",
        "    # 作成した特徴を追加\n",
        "    feature_df = pd.merge(feature_df, user_product_count_df, on=['user-product'], how='left')\n",
        "    \n",
        "    # 中間生成したdfを削除しメモリ解放\n",
        "    del user_product_count_df\n",
        "    gc.collect()\n",
        "    \n",
        "    # 3-3. 用户在过去订单中订购目标产品的百分比\n",
        "    feature_df['user_product_order_rate'] = feature_df['user-product_count'] / feature_df['order_count']\n",
        "    \n",
        "    ######################### 作成したdfを返す ###############################\n",
        "    \n",
        "    return feature_df"
      ],
      "metadata": {
        "id": "JjQAyoTpHps2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orders_prior = orders.query(\"eval_set == 'prior'\")"
      ],
      "metadata": {
        "id": "rbCryuQ2JLm7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "向训练添加特征量"
      ],
      "metadata": {
        "id": "R27JdIa3JTkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_feature_df = add_features(order_products_train, orders_prior, order_products_prior)"
      ],
      "metadata": {
        "id": "yUy7zvctJUBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "向训练添加特征量"
      ],
      "metadata": {
        "id": "8M9oJP9RKBEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_feature_df = add_features(test_df, orders_prior, orders_product_prior)"
      ],
      "metadata": {
        "id": "Rbc47T9dKBcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "导出学习前数据"
      ],
      "metadata": {
        "id": "6LW8tYytKE09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_feature_df.to_csv(\"train_feature.csv\", header=True, index=False)\n",
        "test_feature_df.to_csv(\"test_feature.csv\", header=True, index=False)"
      ],
      "metadata": {
        "id": "i5TsLFRKKHtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_feature_df.head()"
      ],
      "metadata": {
        "id": "BhiUuxAyKKne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "开始学习"
      ],
      "metadata": {
        "id": "KPeb-BxxKN-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "根据所选模型的官方文档修改config中的参数"
      ],
      "metadata": {
        "id": "T1odUEUaKbLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# config\n",
        "CFG = {\n",
        "    \n",
        "    \"seed\":0,\n",
        "    \"fold_num\":5,\n",
        "    \n",
        "    \n",
        "    \"num_leaves\":255, \n",
        "    \"max_depth\":-1, \n",
        "    \"learning_rate\":0.03, \n",
        "    \"n_estimators\":6000, \n",
        "    \"subsample\":0.6, \n",
        "    \"subsample_freq\":1, \n",
        "    \"colsample_bytree\":0.8, \n",
        "    \"objective\":'binary',\n",
        "    \"eval_metric\": 'logloss',\n",
        "    \"early_stopping_rounds\":50, \n",
        "    \"verbose\":100, \n",
        "    \"step\":20, \n",
        "    \n",
        "    \"debug\":False \n",
        "}"
      ],
      "metadata": {
        "id": "I8qAeesJKPq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 予測結果を管理する配列\n",
        "answer = np.array([])\n",
        "answer_train = np.array([])\n",
        "AUC_list = []\n",
        "\n",
        "# 学習用のデータを説明変数と目的変数に分割\n",
        "if CFG['debug']:\n",
        "    full_data_x = train_feature_df.drop([\"order_id\", \"eval_set\", \"product_id\", \"target\" , \"user-product\"],axis=1).iloc[:100000]\n",
        "    full_data_y = train_feature_df[[\"target\"]].iloc[:100000]\n",
        "else:\n",
        "    full_data_x = train_feature_df.drop([\"order_id\", \"eval_set\", \"product_id\", \"target\", \"user-product\"],axis=1)\n",
        "    full_data_y = train_feature_df[[\"target\"]]\n",
        "\n",
        "# test用のデータ\n",
        "test_x = test_feature_df.drop([\"order_id\", \"eval_set\", \"product_id\", \"user_id\", \"user-product\"],axis=1)\n",
        "\n",
        "# GroupKFoldの分割\n",
        "groups = full_data_x['user_id'].values\n",
        "group_kfold = GroupKFold(n_splits=CFG['fold_num'])\n",
        "\n",
        "for i, (train_index, valid_index) in enumerate(group_kfold.split(full_data_x, full_data_y, groups)):\n",
        "    \n",
        "    # デバッグモードの場合は1foldだけ実行して終了\n",
        "    if CFG['debug'] and i == 1:\n",
        "        break\n",
        "    \n",
        "    print(\"TRAIN:\", train_index, \"VALID:\", valid_index)  \n",
        "        \n",
        "    # 学習と検証用にデータ分割\n",
        "    train_x = full_data_x.iloc[train_index].drop([\"user_id\"],axis=1)\n",
        "    valid_x = full_data_x.iloc[valid_index].drop([\"user_id\"],axis=1)\n",
        "    \n",
        "    train_y = full_data_y.iloc[train_index]\n",
        "    valid_y = full_data_y.iloc[valid_index]\n",
        "  \n",
        "    # 回帰予測用のlightgbmモデルを設定\n",
        "    gbm_model = lgb.LGBMClassifier(\n",
        "        boosting_type='gbdt', # 木を作るときのルール\n",
        "        num_leaves=CFG['num_leaves'], # 葉の数\n",
        "        max_depth=CFG['max_depth'], # 最大の深さ\n",
        "        learning_rate=CFG['learning_rate'], # 学習率\n",
        "        n_estimators=CFG['n_estimators'], # 木を作る数\n",
        "        subsample=CFG['subsample'], # 学習に用いるデータの割合\n",
        "        subsample_freq=CFG['subsample_freq'], # サンプリングを行う頻度\n",
        "        colsample_bytree=CFG['colsample_bytree'], # 学習に用いる列の割合\n",
        "        objective=CFG['objective'], # 対象とするのは回帰問題\n",
        "        random_state=CFG['seed'], # 乱数シード\n",
        "        silent=False, # 学習内容の表示\n",
        "        importance_type='gain' # 変数の重要度の計算方法\n",
        "    )\n",
        "\n",
        "    # モデルの学習\n",
        "    gbm_model.fit(train_x, \n",
        "                  train_y,\n",
        "                  eval_set=[(valid_x, valid_y)],\n",
        "                  eval_metric=CFG[\"eval_metric\"],\n",
        "                  early_stopping_rounds=CFG['early_stopping_rounds'],\n",
        "                  verbose=CFG['verbose'])\n",
        "    \n",
        "    # 予測\n",
        "    gbm_pred = gbm_model.predict_proba(valid_x)\n",
        "    \n",
        "    # スコアの計算\n",
        "    AUC = roc_auc_score(valid_y[['target']], gbm_pred[:,1])\n",
        "    print(\"AUC : \", AUC)\n",
        "    AUC_list += [AUC]\n",
        "\n",
        "    # 予測\n",
        "    temp_pred = gbm_model.predict_proba(test_x)\n",
        "    \n",
        "    # 答えの結果を記録\n",
        "    if i == 0:\n",
        "        answer = temp_pred[:, 1]\n",
        "        answer_train = gbm_pred[:, 1]\n",
        "    else:\n",
        "        answer += temp_pred[:, 1]\n",
        "    \n",
        "    # デバッグモードの場合は１ループ目のみ色々可視化\n",
        "    if CFG['debug'] and i == 0:\n",
        "\n",
        "        # 説明変数の重要度を格納するためのデータフレームを作成\n",
        "        feature_importances = pd.DataFrame()\n",
        "        feature_importances['feature'] = train_x.columns\n",
        "        feature_importances['importance'] = gbm_model.feature_importances_\n",
        "    \n",
        "        # 重要度が大きい順に可視化\n",
        "        plt.figure(figsize=(16, 16))\n",
        "        sns.barplot(data=feature_importances.sort_values('importance', ascending=False).head(50),\n",
        "                    x='importance',\n",
        "                    y='feature');\n",
        "        plt.title('50 TOP feature importance')\n",
        "        plt.show()\n",
        "        \n",
        "    # F1スコア\n",
        "    best_F1 = 0\n",
        "    best_thresfold = 0\n",
        "    \n",
        "    valid = pd.concat([full_data_x.iloc[valid_index], full_data_y.iloc[valid_index]], axis=1)\n",
        "    \n",
        "    for thresfold in range(0, CFG['step'], 1):\n",
        "        if thresfold == 0:\n",
        "            continue\n",
        "        if thresfold > 6:\n",
        "            break\n",
        "\n",
        "        F1_sum = 0\n",
        "        F1_num = valid['user_id'].nunique() # ユーザ数\n",
        "        # その閾値での予測値\n",
        "        valid['pred'] = np.where(gbm_pred < thresfold/CFG['step'], 0.0, 1.0)[:,1]\n",
        "        \n",
        "        # ユーザごとのF1スコアを計算し加算\n",
        "        for data in tqdm(valid.groupby('user_id')):\n",
        "            F1_sum += f1_score(data[1][['target']].values, data[1][['pred']].values) \n",
        "        \n",
        "        # F1スコアの平均を算出\n",
        "        F1 = F1_sum / F1_num \n",
        "        \n",
        "        print(\"thresfold:\", thresfold, \" F1:\", F1)\n",
        "        \n",
        "        if F1 > best_F1:\n",
        "            best_F1 = F1\n",
        "            best_thresfold = thresfold/CFG['step']\n",
        "    print(\"bestF1\", best_F1)\n",
        "    print(\"best_thresfold\", best_thresfold)\n",
        "    # 使ったDFの削除\n",
        "    del train_x, valid_x, train_y, valid_y, temp_pred, valid\n",
        "    gc.collect()\n",
        "\n",
        "# 提出用のファイルの作成\n",
        "submission = test_feature_df.copy()\n",
        "pred_test = answer / CFG['fold_num']\n",
        "\n",
        "# 提出用ファイルの作成\n",
        "submission['target'] = pred_test"
      ],
      "metadata": {
        "id": "_7JfpKK6KjcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans = pd.DataFrame()\n",
        "for count, data in enumerate(tqdm(submission.groupby('order_id'))):\n",
        "    \n",
        "    order_dic = {}\n",
        "    order_dic['order_id'] = str(int(data[1].iloc[0]['order_id']))\n",
        "  \n",
        "    first_flg = True\n",
        "    t_str = \"\"\n",
        "    for i in data[1].iterrows():\n",
        "#         print(i[1]['target'])\n",
        "        \n",
        "        if i[1]['target'] > best_thresfold:\n",
        "            if first_flg == True:\n",
        "                t_str += str(i[1]['product_id'])\n",
        "                first_flg = False\n",
        "            else:\n",
        "                t_str += \" \"\n",
        "                t_str += str(i[1]['product_id'])\n",
        "\n",
        "    if first_flg == True:\n",
        "            order_dic['products'] = \"None\"\n",
        "    else:\n",
        "        order_dic['products'] = t_str\n",
        "#     print(order_dic)\n",
        "    ans = ans.append(order_dic, ignore_index=True)\n",
        "    \n",
        "#     if count > 10:\n",
        "#         break\n"
      ],
      "metadata": {
        "id": "vnQo_aEYK-7r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}