{"cells":[{"cell_type":"code","source":["import dlt\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c1339550-66c9-480c-9915-75d5c4d6e355","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["import datetime\nmonth = datetime.datetime.now().month"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aa2cdf52-537e-4b60-851e-ef119e4425b3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["@dlt.table\ndef df_orders():\n    df = (\n        spark.readStream.format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"csv\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"header\", \"true\")\n        .option(\"sep\", \",\")\n        .load(\"s3://yadi-pipeline/month-{}/orders\".format(month))\n    )\n    return (\n        df.withColumn(\"order_id\", col(\"order_id\").cast(IntegerType()))\n        .withColumn(\"user_id\", col(\"user_id\").cast(IntegerType()))\n        .withColumn(\"order_number\", col(\"order_number\").cast(IntegerType()))\n        .withColumn(\"order_dow\", col(\"order_dow\").cast(IntegerType()))\n        .withColumn(\"order_hour_of_day\", col(\"order_hour_of_day\").cast(IntegerType()))\n        .withColumn(\n            \"days_since_prior_order\", col(\"days_since_prior_order\").cast(DoubleType())\n        )\n    )\n\n\ndlt.create_target_table(\"silver_order\")\n\ndlt.apply_changes(\n    target=\"silver_order\",\n    source=\"df_orders\",\n    keys=[\"order_id\"],\n    sequence_by=col(\"order_id\"),\n)\n\n\n@dlt.table\ndef df_products():\n    df = (\n        spark.readStream.format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"csv\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"header\", \"true\")\n        .option(\"sep\", \",\")\n        .load(\"s3://yadi-pipeline/month-{}/products\".format(month))\n    )\n    return (\n        df.withColumn(\"product_id\", col(\"product_id\").cast(IntegerType()))\n        .withColumn(\"aisle_id\", col(\"aisle_id\").cast(IntegerType()))\n        .withColumn(\"department_id\", col(\"department_id\").cast(IntegerType()))\n    )\n\n\ndlt.create_target_table(\"silver_products\")\n\ndlt.apply_changes(\n    target=\"silver_products\",\n    source=\"df_products\",\n    keys=[\"product_id\"],\n    sequence_by=col(\"product_id\"),\n)\n\n\n@dlt.table\ndef df_departments():\n    df = (\n        spark.readStream.format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"csv\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"header\", \"true\")\n        .option(\"sep\", \",\")\n        .load(\"s3://yadi-pipeline/month-{}/departments\".format(month))\n    )\n    return df.withColumn(\"department_id\", col(\"department_id\").cast(IntegerType()))\n\n\ndlt.create_target_table(\"silver_departments\")\n\ndlt.apply_changes(\n    target=\"silver_departments\",\n    source=\"df_departments\",\n    keys=[\"department_id\"],\n    sequence_by=col(\"department_id\"),\n)\n\n\n@dlt.table\ndef df_aisles():\n    df = (\n        spark.readStream.format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"csv\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"header\", \"true\")\n        .option(\"sep\", \",\")\n        .load(\"s3://yadi-pipeline/month-{}/aisles\".format(month))\n    )\n    return df.withColumn(\"aisle_id\", col(\"aisle_id\").cast(IntegerType()))\n\n\ndlt.create_target_table(\"silver_aisles\")\n\ndlt.apply_changes(\n    target=\"silver_aisles\",\n    source=\"df_aisles\",\n    keys=[\"aisle_id\"],\n    sequence_by=col(\"aisle_id\"),\n)\n\n\n@dlt.table\ndef df_prior():\n    df = (\n        spark.readStream.format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"csv\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"header\", \"true\")\n        .option(\"sep\", \",\")\n        .load(\"s3://yadi-pipeline/month-{}/order_products__prior\".format(month))\n    )\n    return (\n        df.withColumn(\"order_id\", col(\"order_id\").cast(IntegerType()))\n        .withColumn(\"product_id\", col(\"product_id\").cast(IntegerType()))\n        .withColumn(\"add_to_cart_order\", col(\"add_to_cart_order\").cast(IntegerType()))\n        .withColumn(\"reordered\", col(\"reordered\").cast(IntegerType()))\n    )\n\n\ndlt.create_target_table(\"silver_prior\")\n\ndlt.apply_changes(\n    target=\"silver_prior\",\n    source=\"df_prior\",\n    keys=[\"order_id\", \"product_id\"],\n    sequence_by=col(\"order_id\"),\n)\n\n\n@dlt.table\ndef df_train():\n    df = (\n        spark.readStream.format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"csv\")\n        .option(\"inferSchema\", \"true\")\n        .option(\"header\", \"true\")\n        .option(\"sep\", \",\")\n        .load(\"s3://yadi-pipeline/month-{}/order_products__train\".format(month))\n    )\n    return (\n        df.withColumn(\"order_id\", col(\"order_id\").cast(IntegerType()))\n        .withColumn(\"product_id\", col(\"product_id\").cast(IntegerType()))\n        .withColumn(\"add_to_cart_order\", col(\"add_to_cart_order\").cast(IntegerType()))\n        .withColumn(\"reordered\", col(\"reordered\").cast(IntegerType()))\n    )\n\n\ndlt.create_target_table(\"silver_train\")\n\ndlt.apply_changes(\n    target=\"silver_train\",\n    source=\"df_train\",\n    keys=[\"order_id\", \"product_id\"],\n    sequence_by=col(\"order_id\"),\n)\n\n\n@dlt.table\ndef df_pad():\n    return (\n        spark.sql(\n            \"select p.product_id, p.product_name, p.aisle_id, a.aisle, p.department_id, d.department from LIVE.silver_products p, LIVE.silver_aisles a, LIVE.silver_departments d \"\n            + \"where p.aisle_id == a.aisle_id and p.department_id == d.department_id \"\n            + \"order by p.product_id\"\n        )\n        .withColumnRenamed(\"aisle\", \"aisle_name\")\n        .withColumnRenamed(\"department\", \"department_name\")\n    )\n\n\n# @dlt.table\n# def df_pads():\n#     return dlt.read(\"df_pad\")\n\n# dlt.create_streaming_live_table(\"silver_pad\")\n\n# dlt.apply_changes(\n#   target = \"silver_pad\",\n#   source = \"df_pads\",\n#   key = [\"product_id\"],\n#   sequence_by = col(\"product_id\")\n# )\n\n\n@dlt.table\ndef df_op():\n    return spark.sql(\n        \"SELECT a.order_id, a.user_id, a.eval_set, a.order_number, a.order_dow, a.order_hour_of_day, a.days_since_prior_order, b.product_id, b.add_to_cart_order, b.reordered FROM LIVE.silver_order a JOIN LIVE.silver_prior b ON a.order_id = b.order_id WHERE a.eval_set = 'prior'\"\n    )\n\n\n# @dlt.table\n# def df_ops():\n#     return dlt.read(\"df_op\")\n\n# dlt.create_streaming_live_table(\"silver_op\")\n\n# dlt.apply_changes(\n#     target = \"silver_op\",\n#     source = \"df_ops\",\n#     key = [\"order_id\", \"product_id\"],\n#     sequence_by = col(\"order_id\")\n# )\n\n\n@dlt.table\ndef df_opp():\n    return spark.sql(\n        \"select p.order_id, o.user_id, p.product_id, a.product_name, a.aisle_id, a.aisle_name, a.department_id, a.department_name, p.add_to_cart_order, p.reordered from LIVE.silver_prior p, LIVE.silver_order o, LIVE.df_pad a \"\n        + \"where p.order_id == o.order_id and p.product_id == a.product_id \"\n        + \"order by p.order_id\"\n    )\n\n\n# @dlt.table\n# def df_opps():\n#     return dlt.read(\"df_opp\")\n\n# dlt.create_streaming_live_table(\"silver_opp\")\n\n# dlt.apply_changes(\n#     target = \"silver_opp\",\n#     source = \"df_opps\",\n#     key = [\"order_id\", \"product_id\"],\n#     sequence_by = col(\"order_id\")\n# )\n\n\n@dlt.table\ndef df_otp():\n    return spark.sql(\n        \"select t.order_id, o.user_id, t.product_id, a.product_name, a.aisle_id, a.aisle_name, a.department_id, a.department_name, t.add_to_cart_order, t.reordered from LIVE.silver_train t, LIVE.silver_order o, LIVE.df_pad a \"\n        + \"where t.order_id == o.order_id and t.product_id == a.product_id \"\n        + \"order by t.order_id\"\n    )\n\n\n# @dlt.table\n# def df_otps():\n#     return dlt.read(\"df_otp\")\n\n# dlt.create_streaming_live_table(\"silver_otp\")\n\n# dlt.apply_changes(\n#     target = \"silver_otp\",\n#     source = \"df_otp\",\n#     key = [\"order_id\", \"product_id\"],\n#     sequence_by = col(\"order_id\")\n# )\n\n\n@dlt.table\ndef user_features_1():\n    return spark.sql(\n        \"SELECT user_id, Max(order_number) AS user_orders, Sum(days_since_prior_order) AS user_period, Avg(days_since_prior_order) AS user_mean_days_since_prior FROM LIVE.silver_order GROUP BY user_id\"\n    )\n\n\n@dlt.table\ndef user_features_2():\n    return spark.sql(\n        \"\"\"SELECT user_id,\n Count(*) AS user_total_products,\n Count(DISTINCT product_id) AS user_distinct_products ,\n Sum(CASE WHEN reordered = 1 THEN 1 ELSE 0 END) / Cast(Sum(CASE WHEN\norder_number > 1 THEN 1 ELSE 0 END) AS DOUBLE) AS user_reorder_ratio\nFROM LIVE.df_op\nGROUP BY user_id \"\"\"\n    )\n\n\n@dlt.table\ndef up_features():\n    return spark.sql(\n        \"SELECT user_id, product_id, Count(*) AS up_orders, Min(order_number) AS up_first_order, Max(order_number) AS up_last_order, Avg(add_to_cart_order) AS up_average_cart_position FROM LIVE.df_op GROUP BY user_id, product_id\"\n    )\n\n\n@dlt.table\ndef prd_features():\n    return spark.sql(\n        \"\"\"SELECT product_id,\n Count(*) AS prod_orders,\n Sum(reordered) AS prod_reorders,\n Sum(CASE WHEN product_seq_time = 1 THEN 1 ELSE 0 END) AS prod_first_orders,\n Sum(CASE WHEN product_seq_time = 2 THEN 1 ELSE 0 END) AS prod_second_orders\nFROM (SELECT *,\n Rank()\n OVER (\n partition BY user_id, product_id\n ORDER BY order_number) AS product_seq_time\n FROM LIVE.df_op)\nGROUP BY product_id \"\"\"\n    )\n\n@dlt.table\ndef user_features():\n    return (spark.sql(\"\"\"SELECT f.user_id, \n                                f.user_orders, \n                                f.user_period, \n                                f.user_mean_days_since_prior, \n                                e.user_total_products, \n                                e.user_distinct_products, \n                                e.user_reorder_ratio\n                         FROM LIVE.user_features_1 f, \n                              LIVE.user_features_2 e \n                         WHERE f.user_id == e.user_id\n                         ORDER BY f.user_id\"\"\"))\n    \n@dlt.table(\npath = \"s3://yadi-pipeline/month-{}/output\".format(month)\n)\n\ndef ml_input():\n    return (spark.sql(\"\"\"SELECT u.product_id, \n                                u.up_orders, \n                                f.user_mean_days_since_prior, \n                                f.user_period, \n                                f.user_distinct_products, \n                                p.prod_second_orders, \n                                p.prod_reorders, \n                                f.user_reorder_ratio, \n                                f.user_total_products, \n                                u.up_average_cart_position, \n                                u.up_first_order, \n                                f.user_orders, \n                                u.up_last_order, \n                                p.prod_orders, \n                                p.prod_first_orders, \n                                u.user_id \n    FROM LIVE.user_features f,  \n         LIVE.prd_features p, \n         LIVE.up_features u \n    WHERE u.user_id == f.user_id\n          and u.product_id == p.product_id \n    ORDER BY u.product_id\"\"\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6ebc0538-925e-47e2-8998-553ffaea7216","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-4030763951663131>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;34m@\u001B[0m\u001B[0mdlt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mdf_orders\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     df = (\n\u001B[1;32m      4\u001B[0m         \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreadStream\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"cloudFiles\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"cloudFiles.format\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"csv\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'dlt' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'dlt' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-4030763951663131>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;34m@\u001B[0m\u001B[0mdlt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mdf_orders\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     df = (\n\u001B[1;32m      4\u001B[0m         \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreadStream\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"cloudFiles\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"cloudFiles.format\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"csv\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'dlt' is not defined"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pipeline-1-1","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4030763951663128}},"nbformat":4,"nbformat_minor":0}
